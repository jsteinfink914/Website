[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Time Series",
    "section": "",
    "text": "Any metric that is measured over regular time intervals makes a Time Series.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Jake",
    "section": "",
    "text": "Currently, I am a graduate student at Georgetown University working towards a degree in the Data Science and Analytics Program. I also majored in Economics with a double minor in Psychology and Business Administration - with a focus in Finance - during my undergraduate time at Georgetown.\n\n\n\n\n\nI spent this summer performing analytical work for the federal contractor, Acumen, to provide support to the Medicare program, specifically by quantifying the savings gained from program integrity efforts. In this role, I worked with large and secure datasets along with languages like SAS and SQL to perform analyses. My work in the M.S. Data Science and Analytics program has given me rigorous analytical skill using statistical and programming knowledge (mainly in Python and R) to perform complex machine learning analyses from the data gathering phase to presentation with supporting visualizations. I am also versed in leveraging cloud computing architectures like Data Bricks and AWS to perform this work.\nOutside of school, I am an avid sports fan, especially of the NBA (if you couldn’t tell from the portfolio) and NFL, and have grown up during the era of analytics and sports merging. Watching that development has deepened my interest in sports and is part of the reason why I decided to pursue Data Science work in the first place."
  },
  {
    "objectID": "Data_Visualization.html",
    "href": "Data_Visualization.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "Taking a look at some of the variables in this dataset shows some of the variables that can have an effect on solar panels. DNI (Direct Normal Irradiance) is a measure of the solar radiation that is directly incident on a surface, without being scattered by the atmosphere. DHI (Diffuse Horizontal Irradiance) is a measure of the solar radiation that is scattered by the atmosphere and reaches a surface indirectly. GHI (Global Horizontal Irradiance) is a measure of the total amount of solar radiation (both direct and diffuse) that is incident on a horizontal surface. It is equal to the sum of DNI and DHI. In other words, GHI takes into account both the direct sunlight and the light that has been scattered and diffused by the atmosphere. Here we see a look at DNI and GHI as well as Temperature and Wind speed, which are known to effect solar panels by reducing efficiency, especially on hot days, and causing an accumulation of debris."
  },
  {
    "objectID": "Data_Visualization.html#exercise",
    "href": "Data_Visualization.html#exercise",
    "title": "Data Vizes in TS",
    "section": "Exercise",
    "text": "Exercise\n\n\n\n\nAdd these text into the Home page and name that page (the title of the page) as “Time Series”.\n\n\nWhat is a Time Series ?\n\nAny metric that is measured over regular time intervals makes a Time Series.\n\nExample: Weather data, Stock prices, Industry forecasts, etc are some of the common ones.\n\nThe analysis of experimental data that have been observed at different points in time leads to new and unique problems in statistical modeling and inference.\nThe obvious correlation introduced by the sampling of adjacent points in time can severely restrict the applicability of the many conventional statistical methods traditionally dependent on the assumption that these adjacent observations are independent and identically distributed.\n\n\n\nData Visualization with Stock Data\n\n\n               MSFT    GOOGL      NFLX      Dates       date\n2012-10-01 24.11845 19.06356  8.007143 2012-10-01 2012-10-01\n2012-10-02 24.25748 18.94369  8.065714 2012-10-02 2012-10-02\n2012-10-03 24.42105 19.08158  8.940000 2012-10-03 2012-10-03\n2012-10-04 24.56009 19.22047  9.524286 2012-10-04 2012-10-04\n2012-10-05 24.41288 19.21046  9.508571 2012-10-05 2012-10-05\n2012-10-08 24.35563 18.96497 10.502857 2012-10-08 2012-10-08\n\n\nWe used the quantmod library to extract the stock prices of 3 large tech companies, Microsoft, Google, and Netflix, from 10/1/2012 - 12/1/2022. After a date conversion this is the resulting time series dataset that we will use to plot below.\n\n\n\n\n\nHere is a static plot of the prices of the 3 stocks over time. THey all follow an upward trend, some more so than others, and all reflect some form of dip during the initial COVID-19 outbreak, but given that they are tech companies which did well during this time, the subsequent gains shadowed the actue drop. Below is the same graph but interactive. Hover over the plot to see the difference."
  },
  {
    "objectID": "Data_Visualization.html#bitcoin-plot-using-plotly",
    "href": "Data_Visualization.html#bitcoin-plot-using-plotly",
    "title": "Data Vizes in TS",
    "section": "Bitcoin plot using plotly",
    "text": "Bitcoin plot using plotly\nWe used the same library to gather Bitcoin prices from 9/15/2021 to today from Yahoo Finance.\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n           rownames.bitc.\n2021-09-15     2021-09-15\n2021-09-16     2021-09-16\n2021-09-17     2021-09-17\n2021-09-20     2021-09-20\n2021-09-21     2021-09-21\n2021-09-22     2021-09-22\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n                 date\n2021-09-15 2021-09-15\n2021-09-16 2021-09-16\n2021-09-17 2021-09-17\n2021-09-20 2021-09-20\n2021-09-21 2021-09-21\n2021-09-22 2021-09-22\n\n\n'data.frame':   335 obs. of  7 variables:\n $ BTC.Open    : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.High    : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Low     : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.Close   : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Volume  : num  102 877 502 110 464 ...\n $ BTC.Adjusted: num  99.7 99.6 99.5 99.6 99.6 ...\n $ date        : Date, format: \"2021-09-15\" \"2021-09-16\" ...\n\n\n\n\n\n\nThis time series plot of Bitcoin prices displays a consistent drop throughout 2022 with a few minor pockets of reprieve. Given the consistent rise of interest rates and Bitcoins position as a “risk-on” asset, it is not shocking to see the negative correlation between the two.\n\n\n\n\n\n\nThis candlestick plot is interactive and allows you to choose a specific date range. The candlestick plot displays 4 pieces of information for each day: the open and closing price make the body of the candlestick, with the whick representing the high and low prices of the day. This is a tool used by traders to understand price movement and volatility.\n\nPlot the climate data (climate.csv) using plotly.\n\nhttps://plotly.com/r/\n\n\n\n\n\n\nHere is a basic time series plot of the Precipitation in DC from 1/2021 to 1/2022. There are 7 weather stations taking measurements in the area each day, so an average was computed for each day and plotted. The red line displays the average rainfall per day across the period. We can see that the most rainfall came from July onward and that the most consistently rainy month was September.\n\nMake only the plots visible in your webpage. (set echo=FALSE in your R code chunck)\nAdd interpretations to all the plots in the webpage.\n\nSo now you will only have the plots and the interpretation in the webpage. You can add titles or can be creative about the page as you want.\n\nUse a different theme than mine. More themes can be found here. https://quarto.org/docs/output-formats/html-themes.html\nAdd this to your Georgetown domain with the title “Data Vizes in TS”. And submit the URL for the Lab 0 assignment.\n\nHowever please remember to take it down at the end of the semester if you don’t need that page on your website.\n\nProfessor James will demonstrate how to push your website to GU domains from your local laptops."
  },
  {
    "objectID": "Labs/Lab 1_Assignment.html#example-1-posixct",
    "href": "Labs/Lab 1_Assignment.html#example-1-posixct",
    "title": "Lab 1 Assignment",
    "section": "Example 1: POSIXct",
    "text": "Example 1: POSIXct\nRead more here: https://www.neonscience.org/resources/learning-hub/tutorials/dc-convert-date-time-posix-r\nPOSIXct is a date and time format in R that stands for “POSIX Coordinated Time”. It is based on the POSIX standard, which defines a representation of time as the number of seconds that have elapsed since a specific reference date, known as the \"epoch\". In R, the epoch is January 1, 1970, 00:00:00 UTC.\nA POSIXct object in R is a numeric value that represents the number of seconds that have elapsed since the epoch, and it is stored as a double-precision floating-point number. It also has a class attribute \"POSIXct\" and a \"POSIXt\" attribute.\n\n# Create a POSIXct object with the current time\ncurrent_time <- Sys.time()\n\n# Print the current time\nprint(current_time)\n\n[1] \"2023-01-18 19:27:20 EST\"\n\n# Output: \"2022-11-26 11:31:05 EST\"\n\nIn this example, the Sys.time() function is used to get the current time and create a POSIXct object. The output is a string representing the current date and time in the local time zone.\nYou can also create POSIXct objects from character strings or numeric values using the as.POSIXct() function.\n\n# Convert a character string to a POSIXct object\ndate_string <- \"2022-01-01 12:00:00\"\ndate_time_object <- as.POSIXct(date_string, format = \"%Y-%m-%d %H:%M:%S\", tz = \"UTC\")\n\n# Print the date and time object\nprint(date_time_object)\n\n[1] \"2022-01-01 12:00:00 UTC\"\n\n# Output: \"2022-01-01 12:00:00 UTC\"\n\nIn this example, the as.POSIXct() function is used to convert a character string “2022-01-01 12:00:00” into a POSIXct object, specifying the format of the input string and the time zone as UTC.\nYou can also convert numeric values to POSIXct object.\n\n# Convert a numeric value to a POSIXct object\nnumeric_value <- 1609459200\ndate_time_object <- as.POSIXct(numeric_value, origin = \"1970-01-01\")\n\n# Print the date and time object\nprint(date_time_object)\n\n[1] \"2020-12-31 19:00:00 EST\"\n\n# Output: \"2022-01-01 00:00:00 EST\"\n\nHere, the as.POSIXct() function is used to convert the numeric value 1609459200 (representing the number of seconds since the epoch) into a POSIXct object with an origin of 1970-01-01.\nPOSIXct objects are useful for working with dates and times, as they allow for easy calculations and comparisons, and are widely used in R for time-series analysis."
  },
  {
    "objectID": "Labs/Lab 1_Assignment.html#example-2-interpret-these-graphs",
    "href": "Labs/Lab 1_Assignment.html#example-2-interpret-these-graphs",
    "title": "Lab 1 Assignment",
    "section": "Example 2: Interpret these graphs",
    "text": "Example 2: Interpret these graphs\nThese are Text book examples; in-built datasets.\nIdentify ts components: trends, sesonality, periodic fluctuations (cyclic) and stationarity.\n\nlibrary(astsa) \n\nplot(jj, type=\"o\", ylab=\"Quarterly Earnings per Share\", main = \"Johnson & Johnson Quarterly Earnings\") \n\n\n\n\n\nautoplot(AirPassengers, title = 'AirPassengers', xlab = 'Year', ylab = 'Passengers')\n\nWarning in ggplot2::geom_line(na.rm = TRUE, ...): Ignoring unknown parameters:\n`title`\n\n\n\n\n\n\nplot(globtemp, type=\"o\", ylab=\"Global Temperature Deviations\", main=\"Global Warming \")\n\n\n\n\n\nautoplot(ustreas) + xlab(\"Day\") + ylab(\"US treasury bill contracts\") \n\n\n\n\n\nautoplot(qauselec) + xlab(\"Year\") + ylab(\"billion kWh\") +ggtitle(\"Quarterly Australian Electricity production\")\n\n\n\n\n\nautoplot(speech, main=\"Speech Data \") \n\n\n\n\nmore: https://cran.r-project.org/web/packages/ggfortify/vignettes/basics.html"
  },
  {
    "objectID": "Labs/Lab 1_Assignment.html#example-3-step-by-step-time-series-decomposition",
    "href": "Labs/Lab 1_Assignment.html#example-3-step-by-step-time-series-decomposition",
    "title": "Lab 1 Assignment",
    "section": "Example 3: Step-by-step Time Series Decomposition",
    "text": "Example 3: Step-by-step Time Series Decomposition\n\nStep 1: Import the Data\n\nAdditive\n\n\n# Additive \n# install.packages(\"fpp\")\nlibrary(fpp)\n\nWarning: package 'fpp' was built under R version 4.2.2\n\n\nLoading required package: lmtest\n\n\nWarning: package 'lmtest' was built under R version 4.2.2\n\n\n\nAttaching package: 'fpp'\n\n\nThe following objects are masked from 'package:fpp2':\n\n    ausair, ausbeer, austa, austourists, debitcards, departures,\n    elecequip, euretail, guinearice, oil, sunspotarea, usmelec\n\n\nThe following object is masked from 'package:astsa':\n\n    oil\n\ndata(ausbeer) # This ausbeer dataset is an example of additive time series \ntimeserie_beer = tail(head(ausbeer, 17*4+2),17*4-4)\nplot(as.ts(timeserie_beer)) # As the metric values increase, the seasonality stays relatively constant.\n\n\n\n\n\nMultiplicative\n\n\n# Multiplicative \n# install.packages(\"Ecdat\")\n#install.packages(\"Ecfun\")\n#library(Ecdat)\n#library(Ecfun)\n\ndata(AirPassengers) # This AirPassengers dataset is an example of multiplicative time series \ntimeserie_air = AirPassengers\nplot(as.ts(timeserie_air)) # The more passengers there are, the more seasonality is observed.\n\n\n\n\n\n\nStep 2: Detect the Trend\n\nAdditive\n\nAustralian beer production clearly follows annual seasonality. As it is recorded quarterly, there are 4 data points recorded per year, and we use a moving average window of 4.\n\n# install.packages(\"forecast\")\nlibrary(forecast)\ntrend_beer = ma(timeserie_beer, order = 4, centre = T)\nplot(as.ts(timeserie_beer))\nlines(trend_beer)\n\n\n\nplot(as.ts(trend_beer))\n\n\n\n\n\nMultiplicative\n\nThe process here is the same as for the additive model. Airline passenger number seasonality also looks annual. However, it is recorded monthly, so we choose a moving average window of 12.\n\n# install.packages(\"forecast\")\nlibrary(forecast)\ntrend_air = ma(timeserie_air, order = 12, centre = T)\nplot(as.ts(timeserie_air))\nlines(trend_air)\n\n\n\nplot(as.ts(trend_air))\n\n\n\n\n\n\nStep 3: Detrend the Time Series\nRemoving the previously calculated trend from the time series will result into a new time series that clearly exposes seasonality\n\nAdditive\n\n\ndetrend_beer = timeserie_beer - trend_beer\nplot(as.ts(detrend_beer))\n\n\n\n\n\nMultiplicative\n\n\ndetrend_air = timeserie_air / trend_air\nplot(as.ts(detrend_air))\n\n\n\n\n\n\nStep 4: Average the Seasonality\nFrom the detrended time series, it’s easy to compute the average seasonality. We add the seasonality together and divide by the seasonality period. Technically speaking, to average together the time series we feed the time series into a matrix. Then, we transform the matrix so each column contains elements of the same period (same day, same month, same quarter, etc…). Finally, we compute the mean of each column. Here is how to do it in R:\n\nAdditive\n\nQuarterly seasonality: we use a matrix of 4 rows. The average seasonality is repeated 16 times to create the graphic to be compared later (see below)\n\nm_beer = t(matrix(data = detrend_beer, nrow = 4))\nseasonal_beer = colMeans(m_beer, na.rm = T)\nplot(as.ts(rep(seasonal_beer,16)))\n\n\n\n\n\nMultiplicative\n\nMonthly seasonality: we use a matrix of 12 rows. The average seasonality is repeated 12 times to create the graphic we will compare later (see below)\n\nm_air = t(matrix(data = detrend_air, nrow = 12))\nseasonal_air = colMeans(m_air, na.rm = T)\nplot(as.ts(rep(seasonal_air,12)))\n\n\n\n\n\n\nStep 5: Examining Remaining Random Noise\nThe previous steps have already extracted most of the data from the original time series, leaving behind only “random” noise.\n\nAdditive\n\nThe additive formula is “Time series = Seasonal + Trend + Random”, which means “Random = Time series – Seasonal – Trend”\n\nrandom_beer = timeserie_beer - trend_beer - seasonal_beer\nplot(as.ts(random_beer))\n\n\n\n\n\nMultiplicative\n\nThe multiplicative formula is “Time series = Seasonal * Trend * Random”, which means “Random = Time series / (Trend * Seasonal)”\n\nrandom_air = timeserie_air / (trend_air * seasonal_air)\nplot(as.ts(random_air))\n\n\n\n\n\n\nStep 6: Reconstruct the Original Signal\nThe decomposed time series can logically be recomposed using the model formula to reproduce the original signal. Some data points will be missing at the beginning and the end of the reconstructed time series, due to the moving average windows which must consume some data before producing average data points.\n\nAdditive\n\nThe additive formula is “Time series = Seasonal + Trend + Random”, which means “Random = Time series – Seasonal – Trend”\n\nrecomposed_beer = trend_beer+seasonal_beer+random_beer\nplot(as.ts(recomposed_beer))\n\n\n\n\n\nMultiplicative\n\nThe multiplicative formula is “Time series = Seasonal * Trend * Random”, which means “Random = Time series / (Trend * Seasonal)”\n\nrecomposed_air = trend_air*seasonal_air*random_air\nplot(as.ts(recomposed_air))"
  },
  {
    "objectID": "Data_Sources.html",
    "href": "Data_Sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "The data sources for this project are 3-fold: Solar radiation and weather data, Solar Energy Generation and Consumption data, and solar stock data."
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "The full code for the project can be found here.\nIn recent years, renewable energy has gained increasing attention as a crucial solution to reduce greenhouse gas emissions and combat climate change. Among the various forms of renewable energy, solar power stands out as one of the most promising options due to its abundance, scalability, and potential for cost-effectiveness. California, known for its commitment to renewable energy, and having the fifth-largest economy in the world, with a GDP of over $3 trillion, has set ambitious goals to transition to clean energy and has already made significant progress in increasing its solar power generation. California ranks first in the nation for installed solar power capacity and generates enough solar energy to power over 5.5 million homes. As a result, it provides a great study area for this analysis.\nThis project aims to focus on analyzing the trends of solar radiation, weather data, solar stock performance, and solar power generation and consumption over time in California. By analyzing time series data, I aim to identify patterns and trends that can inform the optimization of solar power generation and consumption. Furthermore, by including solar stock performance data, the project will also examine the impact of solar power on the economy. With the increasing importance of renewable energy, understanding the trends and patterns in solar power in California can provide valuable insights for policymakers and industry leaders on the potential of solar power and how to improve its effectiveness in the future."
  },
  {
    "objectID": "Introduction.html#key-questions",
    "href": "Introduction.html#key-questions",
    "title": "Introduction",
    "section": "Key Questions",
    "text": "Key Questions\n\nHow does solar radiation in California vary over time?\nHow does weather (e.g., temperature, precipitation, cloud cover) in California affect solar power generation over time?\nIs there a correlation between solar radiation in California and the returns of solar stocks over time?\nIs there a correlation between solar radiation in California and solar power generation over time?\nHow has the amount of solar power generation in California changed over time, and what factors have contributed to this change?\nCan you predict future solar power generation in California using solar radiation, weather, and solar stock data?\nAre there any specific anomalies in the time series data of solar radiation, weather, solar stock returns, and solar power generation in California?\nCan we identify any external factors such as government policies or economic conditions that may have an impact on the trend of solar power generation in California over time?\nAre there any trends in solar panel technology and how do they impact the solar power generation data?"
  },
  {
    "objectID": "Data_Sources.html#solar-radiation-and-weather-data",
    "href": "Data_Sources.html#solar-radiation-and-weather-data",
    "title": "Data Sources",
    "section": "Solar Radiation and Weather Data",
    "text": "Solar Radiation and Weather Data\nUsing the National Solar Radiation Database and the NSRDB Viewer within, a point in the San Joaquin Valley (the most fertile farming area in the U.S.) was selected which allowed for the extraction of solar radiation and weather variables for the location at daily frequency from 1998 - 2020. The exact data can be downloaded here. Data for any longitude and latitude coordinates can be extracted using this site. A look at the solar radiation data can be seen below:"
  },
  {
    "objectID": "Data_Sources.html#california-energy-generation-and-consumption",
    "href": "Data_Sources.html#california-energy-generation-and-consumption",
    "title": "Data Sources",
    "section": "California Energy Generation and Consumption",
    "text": "California Energy Generation and Consumption\nThe U.S. Energy Information Administration stores information on all forms of energy generation and consumption. It offers an API that allows for the pulling of up to 5000 rows of data at a time. They also have a point and click interface that helps craft the API for you which can be found here. The code used to unpack the JSON is below:\n\n# Define the API endpoint and parameters\nendpoint <- 'https://api.eia.gov/v2/electricity/electric-power-operational-data/data/?frequency=monthly&data[0]=ash-content&data[1]=consumption-for-eg&data[2]=consumption-for-eg-btu&data[3]=consumption-uto&data[4]=consumption-uto-btu&data[5]=cost&data[6]=cost-per-btu&data[7]=generation&data[8]=heat-content&data[9]=receipts&data[10]=receipts-btu&data[11]=stocks&data[12]=sulfur-content&data[13]=total-consumption&data[14]=total-consumption-btu&facets[sectorid][]=98&facets[fueltypeid][]=AOR&facets[fueltypeid][]=NG&facets[fueltypeid][]=SUN&facets[fueltypeid][]=WND&facets[location][]=CA&start=2001-01&end=2022-10&sort[0][column]=period&sort[0][direction]=desc&offset=0&length=5000&api_key=F2M2Ra1oc6mkOx8oTyPnjmCHYN3R5fm12Bkey5we'\n# Send the GET request\nresponse <- GET(endpoint)\n\n# Parse the JSON response\ndata <- content(response)\n\n##Rowbind all the json items into a df\nconsumption_raw <- as.data.frame(do.call(rbind, data$response$data))\n\n##Replace Null values with NA's in each column\nconsumption_lists <- \n  lapply(consumption_raw, function(x) {\n    lapply(x, function(y) {\n      ifelse(is.null(y), NA, y)\n    })\n  }\n  )\n## Convert columns from lists to vectors\nconsumption <- data.frame(lapply(consumption_lists, function(y){Reduce(c, y)}))\n\n## Write to csv to avoid recalling the API\nwrite.csv(consumption, 'data/consumption_CA_2001_2022.csv', row.names = FALSE)\n\nNow we can take a quick look at the data. Notably, the solar energy consumption is larger then the utility scale power generation of solar energy. This is due to the widespread use of household solar panels to provide homes with energy."
  },
  {
    "objectID": "Data_Sources.html#stock-data",
    "href": "Data_Sources.html#stock-data",
    "title": "Data Sources",
    "section": "Stock Data",
    "text": "Stock Data\nLastly, I will be looking at stock data using the quantmod library and yahoo finance. This data will be used to look at the performance of solar energy companies, particularly ones that focus on serving California residents. Using this data, it will be possible to track the financial performance of these companies over time and see how it relates to power generation and other variables like solar radiation and weather."
  },
  {
    "objectID": "Data_Visualization.html#california-energy-generation-and-consumption",
    "href": "Data_Visualization.html#california-energy-generation-and-consumption",
    "title": "Data Vizes in TS",
    "section": "California Energy Generation and Consumption",
    "text": "California Energy Generation and Consumption\nPreviously, we saw the plots of solar energy generation and consumption and their rapid growth over time. How, has this relationship changed over time? The plot below gives insight into a gradual, but quickening trend: the increase in utility scale solar power generation as a proportion of total consumption. The piece wise nature of the graph is odd, as it indicates the ratio remains constant within years, but decreases across years. Regardless, the overall trend is clear that utility scale solar generation is taking more share of total consumption over time, and this can be expected to continue."
  },
  {
    "objectID": "Data_Visualization.html#solar-stocks",
    "href": "Data_Visualization.html#solar-stocks",
    "title": "Data Vizes in TS",
    "section": "Solar Stocks",
    "text": "Solar Stocks\nPrices of stocks can tell a lot about trends over time and the nominal number that each share is worth. But looking at daily returns give us an insight into the fluctuations in stock prices over time and provide a clearer picture of how the stocks have performed relative to each other, as well as how emotional the day to day life of a shareholder can be. By clicking on the legend, you can isolate a stock and see its daily returns."
  },
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "For Consistency, all data from here on out will be from 2006-2020 to keep time spans consistent."
  },
  {
    "objectID": "EDA.html#solar-radiation-and-weather-data",
    "href": "EDA.html#solar-radiation-and-weather-data",
    "title": "EDA",
    "section": "Solar Radiation and Weather Data",
    "text": "Solar Radiation and Weather Data\n\nTime Series of GHI\nHere we examine the main form of solar radiation of interest to us, GHI - solar radiation that solar panels make use of.\n\n\n\n\n\n\nTrend: From the plot we can see something very good for all of us, a stable and consistent seasonal pattern. This pattern remains consistent over time indicating no major trend.\nSeasonality: We can see consistent peaks in GHI during the summer time, always peaking in June around the summer solstice, and always at a trough around the winter solstice in December.\nPeriodic Variation: There are plenty of instances of variation from the overall seasonal cycle that can be caused by a few factors. Clouds can block or scatter the sun’s rays, causing changes in the amount of GHI. Atmospheric conditions like dust, pollution, and other particles in the air can affect the amount of GHI that reaches the Earth’s surface. Weather patterns such as storms, high-pressure systems, and fronts can cause short-term fluctuations. Lastly, the sun operates on 11-year solar cycles, during which the sun’s activity changes due to the number of sunspots, magnetic storms, and solar flares, which can impact the amount of energy and particles that the sun releases into space.\nAdditive Time Series:This is an additive time series as we see constant changes over time, not exponential growth or decay. Because there is no clear trend over the time span and variance appears to be relatively equal, we can say with confidence that this is additive.\n\n\nLag Plots\nLag plots are a useful tool for visualizing patterns in time series data. They help to detect the presence of autocorrelation, which occurs when the value of a time series at a given point is related to the values at previous points.\nIn a lag plot, each data point is plotted against a lagged version of itself, with the x-axis representing the original time series values and the y-axis representing the lagged values or vice versa. The resulting scatter plot can then be used to identify any patterns or relationships between the original and lagged values.\nLag plots are useful because they provide a visual representation of the relationships between the values of a time series, which can be helpful in identifying patterns such as seasonality, trend, or autocorrelation. Autocorrelation can have a significant impact on time series forecasting, as it can indicate that the values of the series are dependent on previous values and not just random fluctuations.\nBelow is a lag plot of GHI:\n\n\n\n\n\nHere we see some expected patterns arising. A lag of 1 day exhibits very high positive correlation indicating that the yesterday’s solar radiation is useful for predicting todays. At a lag of 1 month, we see strong positive correlation but certainly less than at one day. A one month lag is usually within the same season and thus has similar dynamics with regard to orientation of the earth and sun. At a 3 month lag we see no correlation as these reflect the relationship of solar intensity between adjacent seasons which can be either positive or negative. At 6 months, we see a strong negative trend because halfway around the calendar, solar radiation is moving the opposite direction - for instance decreasing radiation in the winter compared to increasing radiation in the summer. At 9 months we see a repeat of the relationship at 3 months for the same reasons, it is just the next season’s data from the prior year (winter 2015 on spring 2014, etc.). Lastly at 360, nearly a full year, we see a strong positive correlation again.\n\n\nDecomposition\n\n\n\n\n\n\nThis additive decomposition gives insight into solar radiation. We see a very consistent trend rangebound within approximately 15 GWh over the 14 years. The plot also makes clear the consistent seasonal element of GHI fluctuating between peaks and troughs in the summer and winter respectively.\n\n\nMoving Average Smoothing\n\n\n\n\n\n\nMoving averages are another really useful way to identify trends from the overall data. They work by sliding an averaging window over the data. For instance, the 30 day MA is claculated by taking the average of the 30 data points around the target data point (15 before and 15 after). This has the effect of removing noise from the data. The MA plot here shows more of the same, we can clearly see the cyclical/seasonal nature of the data based on the 30 and 180 day MA’s and the 1 year MA shows a truly flat trend in the context of the data as a whole.\n\n\nACF and PACF\nACF (Autocorrelation Function) and PACF (Partial Autocorrelation Function) are two commonly used plots in time series analysis to help identify the underlying structure of a time series.\nThe ACF plot shows the correlation between a time series and lagged versions of itself, which helps to identify if there is a pattern in the time series that repeats itself over time. If there is a strong correlation at a specific lag, it suggests that there is an autocorrelation in the series.\nOn the other hand, the PACF plot shows the amount of correlation between a time series and lagged versions of itself after removing the effect of intermediate lags. It can help identify the number of autoregressive (AR) terms in an ARIMA (AutoRegressive Integrated Moving Average) model, which is a common time series forecasting model. The partial autocorrelation plot shows the correlation between a time series and its lagged values, but with the effects of previous lags removed.\n\n\n\n\n\nThe ACF plot once again displays the intense seasonality, with strong autocorrelations at almost every lag. The PACF shows strong correlation for the first handful of lags and then becomes insignificant afterwards. These plots clearly display a non stationary time series as there are intense autocorrelations amongst lagged values.\n\n\nStationarity Test\nAn Augmented Dickey Fuller Test shows that this time series is not stationary, which can be clearly seen through the ACF plot referenced above. The reason this test is so close to confirming stationarity (technically a p-value of below .05 is all that is needed) is because the test is largely concerned with seeing a consistent mean throughout the time series, which the GHI time series appears to have. Because the main non-stationarity is a result of seasonality, we also use a KPSS test which examines this element of the time series, and here we see a more clear rejection of stationarity.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  radiation\nDickey-Fuller = -3.3957, Lag order = 17, p-value = 0.05411\nalternative hypothesis: stationary\n\n\n\n    KPSS Test for Level Stationarity\n\ndata:  radiation\nKPSS Level = 0.070398, Truncation lag parameter = 10, p-value = 0.1\n\n\n\n\nMaking Solar Data Stationary\nTo make the data stationary, we make use of first differencing, which follows the formula: \\[\\hat{y_t} = y_t - y_{t-1} \\] In doing so we are looking at the change between the current and previous value, which removes trend and seasonality from the data. After applying this transformation we can see the new data:\n\n\n\n\n\n\n\n\nUpdated ACF and PACF\n\n\n\n\n\nThe new ACF and PACF plots show how significant this transformation was from a stationarity perspective. Here the ACF shows a significant autocorrelation at a lag of 1 (because it incorporates the previous term in differencing) and then the correlations become insignificant noise. The PACF largely does not change from above, but this transformation nonetheless has taken the data from seasonal to stationary.\n\n\nUpdated Stationarity Tests\nFor a sanity check, we see that the new data is now described as stationary.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  radiation\nDickey-Fuller = -3.3957, Lag order = 17, p-value = 0.05411\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  solar_ts\nDickey-Fuller = -21.578, Lag order = 17, p-value = 0.01\nalternative hypothesis: stationary"
  },
  {
    "objectID": "EDA.html#california-energy-generation-and-consumption",
    "href": "EDA.html#california-energy-generation-and-consumption",
    "title": "EDA",
    "section": "California Energy Generation and Consumption",
    "text": "California Energy Generation and Consumption\n\nTime Series of CA Solar Energy Consumption\nFor the rest of this project, we will be focusing solely on the solar energy consumption in CA, as it provides a more comprehensive picture of solar energy use then utility scale generation.\n\n\n\n\n\n\nTrend: There is a clear positive trend in the data as solar energy adoption increases over time. This trend appears to be exponential and began to really take off around 2014. California has a long history of providing incentives for renewable energy, including solar. In 2013, the state introduced the California Solar Initiative, which provided rebates and incentives to help offset the cost of solar installations. This combined with declining prices, and public demand in a liberal market caused the explosion in solar demand we see.\nSeasonality: The seasonality element of solar energy consumption cannot be ignored. Production and therefore consumption is constrained mainly by solar radiation, which we saw with GHI above is highly seasonal. As a result, most of the power is produced and consumed in the summer months. This is a big limitation on solar power currently, as energy demands usually increase in the winter.\nPeriodic Variation: There are plenty of instances of variation from the overall seasonal cycle that can be caused by a few factors. Energy prices of alternatives can lead to an increase or decrease from the norm. Economic cycles can also contribute to energy demand as well as the adoption rate of solar panels - these usually work together as people will not invest in solar panels as readily in a recession. Additionally, the same factors that can impact GHI like cloud cover, atmospheric conditions, and solar cycles can impact generation and thus consumption.\nMultiplicative Time Series:This is a multiplicative time series as we see proportional, instead of constant changes over time. In other words, given the exponentially increasing trend and the fact that the trend and seasonality variations increase as the magnitude of the data does. In other words as the trend increases, so do the seasonal peaks and troughs.\n\n\nLag Plots\n\n\n\n\n\nThe lag plots begin to show what was clear from the original plot - that this data is non-stationary. At all lags there are positive correlations for each month indicating autocorrelation withing the series. Correlations are strongest at a lag of 1, 12 and 24 as these represent adjacent months, and then calendar year lags. At lags of 3,6 and 9 months we still surprisingly see positive correlation, indicating that despite seasonality, the trend is so strongly positive that it overcomes the usual peak and trough pattern occurring throughout the seasons.\n\n\nDecomposition\n\n\n\n\n\n\nA multiplicative decompostion was used to model this data as discussed above. We can see that after 2014 these residuals flip positive instead of negative, as that is when the exponential trend really starts to lift off. We can see the consistent seasonal pattern and the noise located around a mean of 1 which lends credence to the choice of a multiplicative decomposition.\n\n\nMoving Average Smoothing\n\n\n\n\n\n\nThe MA plot makes clear this exponential trend The 3 and 6 month MA’s show the increasing trend along with the seasonality of the data while the 1 year MA shows only the trend which has begun to level off from its previous exponential growth since 2014. Without the noise we can see that the seasonal fluctuations have become a bit more intense as the trend has increased indicating that this will be an engineering problem that will have to be looked at closely or complimented with other energy sources to lead to a fully sustainable energy source.\n\n\nACF and PACF\n\n\n\n\n\nThe ACF and PACF plots show the lack of stationarity in this data, with strong postive correlations at all lags in the ACF. The PACF decays as the lags get longer which is typical given the ACF plot tailing off in correlation in step with the lags.\n\n\nStationarity Test\nThe stationarity test confirms our suspicions that the data is in fact not stationary.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  cons\nDickey-Fuller = -3.1473, Lag order = 5, p-value = 0.09869\nalternative hypothesis: stationary\n\n\n\n\nMaking Solar Energy Consumption Stationary\nTo make the data stationary we use a similar but slightly different technique from the solar data. We use differencing to remove the seasonality and trend, but first we take the log of the data. This is because with a multiplicative time series and an exponentially increasing trend, the variance in the differences is too variable especially as the trend takes off. A fantastic way to remove the heteroscedasticity and look at the data as percentage differences instead of absolute differences is to take the log.\n\n\n\n\n\n\n\n\nUpdated ACF and PACF\n\n\n\n\n\nThe new ACF plot shows the large effect this has on the data as now autocorrelation is almost entirely removed except at lags of full years. This data is significantly more stationary, with more constant mean and variance over time.\n\n\nUpdated Statitonarity Tets\nThe augmented dickey fuller test once again confirms this change successfully made the data stationary.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  cons\nDickey-Fuller = -3.1473, Lag order = 5, p-value = 0.09869\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  cons_d_ts\nDickey-Fuller = -8.0818, Lag order = 5, p-value = 0.01\nalternative hypothesis: stationary"
  },
  {
    "objectID": "EDA.html#solar-stocks---spwr",
    "href": "EDA.html#solar-stocks---spwr",
    "title": "EDA",
    "section": "Solar Stocks - SPWR",
    "text": "Solar Stocks - SPWR\nFor the sake of simplicity, SPWR will be the chosen stock for this analysis. SunPower has a strong presence in the state of CA, with a history of providing high-quality solar panels and excellent customer service. Additionally, the company’s focus on innovation and commitment to sustainability positions it well in a market that values efficiency, durability, and environmentally-friendly solutions. Furthermore, SunPower’s strong brand recognition and reputation has made it an industry leader in the state. Below is their stock price chart:\n\n\n\n\n\n\nTrend: SPWR stock seems to indicate no real trend although there are subtrends within the plot that are quite strong. The run up pre-2008 coincided with immense optimism about renewable energy, which was then quickly sapped after the Great Financial Crisis. The stock remained relatively quiet until 2014 when another jump occurred likely coinciding with the increase in solar energy demand in CA as a result of the California Solar Initiative. This was followed by another downtrend until a massive spike after the lax monetary policy brought on by the COVID-19 response by the Federal Reserve which saw the entire market rise\nSeasonality: Seasonal factors can also play a role in the stock prices of solar companies. For example, the demand for solar panels and other solar technologies may be higher in the summer months, when there is more daylight and higher levels of solar irradiation, leading to higher stock prices for solar companies during this time. In the case of SPWR, it is not clear from this view that the stock exhibits seasonality.\nPeriodic Variation: Stock prices are enormously volatile and can go off trend at a moment’s notice. Economic indicators, such as gross domestic product (GDP) growth, unemployment rates, and inflation, can have a significant impact on stock prices. We see an example of the economy impacting the stock price after 2008 and the GFC. C Political events, such as elections, government policies, and geopolitical tensions, can also impact stock prices, like we see here in 2014 after the passage of the California Solar Initiative. Market sentiment, or the overall mood and attitude of investors, can also play a role in stock price variations. This is particularly applicable to solar energy as ESG investments usually do better in less cautious times.\nMultiplicative Time Series: This will be treated as a multiplicative time series as stocks usually experience exponential growth or decay, although SPWR has largely remained flat over its entire public market experience.\n\nLag Plots\n\n\n\n\n\nThe patterns in this lag plot are qutie artistic! We see some positive autocorrelation in prices up to about 90 days before a significant tailoff to no autocorrelation between the prices. The strong autocorrelation at a lag of 1 day is the main takeaway of this plot and a sign that this data is not stationary.\n\n\nDecomposition\n\n\n\n\n\n\nThe decomposition shows us two trend bumps coinciding with price spikes. The plot does give insight into the seasonal nature of the stock, which is much more consistent then what originally met the eye. We see consistent price increases taking place from the beginning of the year to 1/3 of the way (around April) before peaking and declining for the rest of the year. This pattern makes sense as financial markets usually project 6 months out, so if we know that solar consumption increases through the summer and peaks in the late summer, then the stocks in this space should increase from January through March.\n\n\nMoving Average Smoothing\n\n\n\n\n\n\nSPWR stock has been on a downward trend since 2006. The 60 and 180 day MA plots show some of the more major fluctuations the stock has seen, mostly before the GCF, and the 1 year MA shows a very smooth trendline on a downward trajectory. Despite the great momentum solar energy has seen, SPWR has not been able to reap the benefits of this.\n\n\nACF and PACF\n\n\n\n\n\nThe ACF and PACF plot shows just how non-stationary the stock data is with significant auto correlations for lags all the way up to a year and beyond. Stock data is known to be non-stationary so this and the test below are just confirmation.\n\n\nStationarity Test\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  stock_ts\nDickey-Fuller = -2.6121, Lag order = 15, p-value = 0.3191\nalternative hypothesis: stationary\n\n\n\n\nMaking it stationary\nFirst differencing is used to remove trend and seasonality. Just like with the consumption data we take the Log first to remove heteroscedasticity. Taking the log difference in stocks is particularly common and important to make the data symmetric around 0. Arithmetic returns are biased in that if you gain 100% on a stock and then lose 50% you make a full round trip, although intuitively you would think the average return is 25%. Taking the log of the price differences fixes this problem so that if your log return is 100% it will take a -100% return to make a full round trip. This is both helpful for interpretation and for the mathematical models as positive and negative numbers can be treated as equal magnitudes.\n\n\n\n\n\n\n\n\nUpdated ACF and PACF Plots\n\n\n\n\n\nAfter log differencing, we see fully stationary data, with essentially no autocorrelation. This is the ideal outcome of making data stationary, with all lags within the bounds of insignificance.\n\n\nUpdated Stationarity Tets\nThe updated augmented dickey fuller test confirms this result.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  stock_ts\nDickey-Fuller = -2.6121, Lag order = 15, p-value = 0.3191\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  stock_log_ts\nDickey-Fuller = -17.75, Lag order = 17, p-value = 0.01\nalternative hypothesis: stationary"
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA.html",
    "href": "ARMA_ARIMA_SARIMA.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "In this section our focus is on fitting ARIMA and SARIMA models to our data. In the previous EDA section, we made our data stationary which prepared it for model fitting. Now we will explore these models and choose the optimal ones to fit our data. GHI is highly seasonal, so we will first start with the typical ARIMA model, and then explore SARIMA models.\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the ACF and PACF plots we can determine parameters for ARIMA models. Two different scales are shown, one that accounts for the full 365 day period (one year) and one up to a lag of 45.\nARIMA is built off of 3 terms (p,d,q). Since the data is already differenced and stationary d = 0. However, when we fit the model using the raw data we would set d = 1 to take the first difference of the data.\np is the number of autoregressive terms to use, ie. the number of previous terms included in the regression. We identify p from the PACF plots as that shows the independent contribution of each lag to autocorrelation. As a result, any lags that show significant independent autocorrelation are candidates for p. In this case candidates include all the way to about 130ish due to the seasonality of the data. A Sarima model will be best for this data but for the ARIMA paradigm we can’t include that many terms. For now we will try 1-11.\nq is the number of lagged noise terms to include. For instance, if q = 2 the noise terms are computed by fitting an AR model with p lagged terms for \\(Y_{t-1}\\) and \\(Y_{t-2}\\). The noise terms are the residuals from these models which is then brought in to the ARIMA regression as a variable. To determine which q to choose, we look at the significant lags in the ACF plot. In this case, potential options for q are 1,2, and 4.\n\n\n\n\n\n\n \n  \n      \n    p \n    d \n    q \n    AIC \n    BIC \n    AICc \n  \n \n\n  \n    43 \n    8 \n    1 \n    2 \n    54802.84 \n    54875.52 \n    54802.89 \n  \n  \n    23 \n    4 \n    1 \n    2 \n    54811.02 \n    54857.28 \n    54811.04 \n  \n  \n    431 \n    8 \n    1 \n    2 \n    54802.84 \n    54875.52 \n    54802.89 \n  \n\n\n\n\n\nBy running through all the possible models we can use information criterion metrics like AIC, BIC, and AICc to analyze the best model. These metrics are derived from information theory and are meant to estimate test error. In the case of these 3, the goal is to find the model with the minimum value. From our results, we can see that ARIMA(8,1,2) has the lowest AIC and AICc, while ARIMA(4,1,2) has the lowest BIC value. It is important to note that BIC imposes a harsher penalty on having more parameters, so this metric will usually return a simpler model then the others.\n\n\n\n\n\n\n\n\n\n\n\nLooking at the model diagnostics of both, we see that there is little autocorrelation among the residuals according to the ACF. The Ljung-box statistic largely confirms this. What this plot shows is the p-value resulting from a hypothesis test where the null is that the residuals at each lag are not autocorrelated - a value below .05 indicates a rejection of this hypothesis. We see that ARIMA (8,1,2) performs better than ARIMA (4,1,2) with nearly all lags failing to reject, while some lags do reject in ARIMA(4,1,2). Unfortunately, both plots show that the residuals are not normally distributed - ideally they would be, which could indicate that there are better models for this data like sarima which takes into account seasonality. Although ARIMA(8,1,2) performed slightly better in terms of lagged autocorrelation of residuals, the parsimony principle combined with the BIC metric favoring ARIMA(4,1,2) leads me to continue with this model. Simpler models, if performance difference is negligible, is always preferred.\n\n\n\nGiven the model we can write our equation: \\[(1-B)y_t = c + (1 + \\phi_1 B + \\phi_2 B^2 + \\phi_3 B^3 + \\phi_4 B^4) y_{t-1} + (1 + \\theta_1 B + \\theta_2 B^2) \\varepsilon_t\\] where \\(B\\) is the lag operator, where \\(B^ky_t = y_{t-k}\\), \\(\\phi\\) represents the AR terms \\(\\theta\\) represents the noise (MA) terms, \\(\\varepsilon_t\\) represents the error, and \\(c\\) represents the intercept.\n\n\n\n\n\nSeries: solar_orig_ts \nARIMA(4,1,3) \n\nCoefficients:\n         ar1     ar2      ar3      ar4      ma1      ma2     ma3\n      0.7042  0.6021  -0.2478  -0.0702  -1.2691  -0.3435  0.6197\ns.e.  0.1381  0.1900   0.0479   0.0167   0.1380   0.2662  0.1290\n\nsigma^2 = 1303:  log likelihood = -27396.22\nAIC=54808.45   AICc=54808.47   BIC=54861.31\n\n\nAuto.arima computed the optimal model as ARIMA(4,1,3) indicating that it would include one extra MA term in the model. The reason for the divergence is that auto.arima uses a stepwise approach to search for the optimal ARIMA model, based on the AIC or BIC criteria. Specifically, auto.arima() starts with a simple ARIMA model, and then iteratively adds AR or MA terms or increases the order of differencing until no further improvement in the AIC or BIC criterion is achieved. Because of this difference in logical framework, auto.arima will often return a slightly different model. In this case the one extra noise term does not require a significant rethinking of our current findings.\n\n\n\n\n\n\n\n\n\nHere we are plotting the predictions from the ARIMA model on the training data over the actual data. The model does a great job of staying close to the data which gives us confidence that this model is complex enough to handle the patterns we see from GHI.\n\n\n\n\n\n\n\n\nPlotting the forecast we see a continued down trend in the data but at a slowing rate before it would reverse for the warmer months. However, this behavior is not likely for long forecasts because the model has no sense of seasonality, which is clearly present in the data. As stated above, we need a sarima model. The bands represent the 80% and 95% CI’s respectively, and we can see they occupy quite a large range, indicating that the model is not adept for making a long term prediction. This was a forecast for 100 days, and we would see much smaller CI’s for a forecast of say 20 days.\n\n\n\n\n\n\n\n \n  \n    Benchmark \n    ME \n    RMSE \n    MAE \n    MPE \n    MAPE \n  \n \n\n  \n    Fitted ARIMA \n    -12.67 \n    28.91 \n    20.16 \n    -18.16 \n    21.71 \n  \n  \n    Mean \n    -80.23 \n    91.27 \n    81.49 \n    -74.47 \n    74.99 \n  \n  \n    Naive \n    -95.34 \n    104.8 \n    95.56 \n    -86 \n    86.09 \n  \n  \n    Drift \n    -97.55 \n    107.27 \n    97.77 \n    -87.96 \n    88.05 \n  \n  \n    sNaive \n    1.31 \n    37.59 \n    27.58 \n    0.45 \n    22.2 \n  \n\n\n\n\n\nTo compare the model to benchmark methods, we split the data into a train and test set. In time series, the test set is the most recent collection of data, so here the train set is assigned as 2006-01-01 to 2020-09-22, with the test set being 2020-09-23 to 2020-12-31. Comparing the ARIMA model to benchmark methods over a 100 day prediction window, we see that the fitted ARIMA blows the other models out of the water by all metrics, with the exception of sNaive which does a decent job. This is exactly what we would want to see, as failing to outperform the benchmarks would mean this model is essentially useless. Here the benchmarks are Mean (predicting future values as the mean of the series), Naive (predictions are the last value in the series), Drift (predictions are a random walk with a drift term), and sNaive (predictions are values from the same period a year ago).\n\n\n\n\n\n\nTo show the results of each method more clearly, I cutoff a large chunk of the GHI data, but all forecast models are based on the entire train set as described above. Here, we see how the ARIMA model and sNaive are the only ones capable of somewhat tracking the downward trend. Although sNaive appears to be super effective for seasonal data, the main issue is that it copies and paste the exact replica of last year data. In real life, patterns rarely repeat exactly, and the ARIMA is a more generalizable model which has better predictive value as a result.\n\n\n\n\nGiven what we know about GHI and the fact that it is highly seasonal, we are going to take a look at seasonal differencing instead of first differencing.\n\n\n\n\n\n\nSimilar to the first differenced GHI, this data looks very stationary which is exactly where we want to be.\n\n\n\n\n\nLooking at the ACF and PACF plots for a SARIMA model, we now not only have to pay attention to the typical lags, but also the key seasonal lags. In this case, a period is 365 for the solar data so we want to look at lags of 365, 730, 1095 to determine options for P and Q. In addition to trying parameters for p as 1-8, and q as 1,2 we will also examine them in conjunction with P’s of 1-5 and a Q of 1.\n\n\nOne issue in R with Sarima model selection by hand is that the maximum supported lag for fitting a SARIMA function by hand is 350. Given that our data is daily with a period taking place over 365 days, this poses a real issue for doing model fitting by hand. As a result, we have to rely upon auto.arima to do the calculation for us.\n\n\nSeries: solar_orig_ts \nARIMA(1,0,1)(0,1,0)[365] \n\nCoefficients:\n         ar1      ma1\n      0.5319  -0.1499\ns.e.  0.0286   0.0335\n\nsigma^2 = 2392:  log likelihood = -27127.72\nAIC=54261.43   AICc=54261.43   BIC=54281.05\n\n\nThe function returns a SARIMA(1,0,1)(0,1,0)[365] indicating that we do only have to use seasonal differencing, but with no seasonal lagged variables or noise - surprising given the ACF and PACF plots. It also incorporates 1 AR terms and 1 MA term from a typical ARIMA model.\n\n\n\n\n\n\n\n\nBased on the residuals plots we can see that this model does not do a great job of capturing the data, and is in large part due to the fact that the SARIMA models in R incorporating any Seasonal AR or MA terms for a lag past 350 fail to compute. As a result, we are left with a basic model, that has low computational complexity, but fails to do justice to the data.\n\n\n\nGiven the model we can write our equation: \\[y_t =  \\frac{(1 + \\theta_1 B)\\epsilon_t}{(1 - \\phi_1 B)(1 - \\Phi_1 B^{365})}\\] where \\(B\\) is the lag operator, where \\(B^ky_t = y_{t-k}\\), \\(\\phi\\) represents the AR terms \\(\\theta\\) represents the noise (MA) terms, \\(\\varepsilon_t\\) represents the error, and \\(c\\) represents the intercept.\n\n\n\n\n\n\n\n\n\nHere we are plotting the predictions from the ARIMA model on the training data over the actual data. The model does a decent enough job making predictions from the train data, but we will have to hold out judgement until we see how well it generalizes to unseen data.\n\n\n\n\n\n\n\n\nPlotting the forecast we see a continuation of the seasonal pattern, a marked improvement over the ARIMA model as it is able to capture the seasonality in the data. We also see tighter standard errors than what we saw from the ARIMA model which is great to see.\n\n\n\n\n\n\n\n \n  \n    Benchmark \n    ME \n    RMSE \n    MAE \n    MPE \n    MAPE \n  \n \n\n  \n    Fitted SARIMA \n    1.31 \n    37.59 \n    27.58 \n    0.45 \n    22.2 \n  \n  \n    Fitted ARIMA \n    -12.67 \n    28.91 \n    20.16 \n    -18.16 \n    21.71 \n  \n  \n    Mean \n    -80.23 \n    91.27 \n    81.49 \n    -74.47 \n    74.99 \n  \n  \n    Naive \n    -95.34 \n    104.8 \n    95.56 \n    -86 \n    86.09 \n  \n  \n    Drift \n    -97.55 \n    107.27 \n    97.77 \n    -87.96 \n    88.05 \n  \n  \n    sNaive \n    1.31 \n    37.59 \n    27.58 \n    0.45 \n    22.2 \n  \n\n\n\n\n\nOne interesting phenomena we are seeing is that the results of the SARIMA model and the sNaive model are essentially identical. Upon further checks, this is not actually the case as some of the data points are different, but for all intents and purposes the model performs the same. This is interesting but not necessarily shocking. Remember that sNaive uses the values from the previous seasonal period (so in this case 365 days ago). The reason for this is due to a property that we should all be thankful for - the extreme consistency of solar radiation over time.\n\n\n\n\n\n\nPlotting the predictions against eachother, we see how similar sNaive and the SARIMA fit are. In this case the ARIMA model does outperform sNaive and SARIMA over the test period, but as we had previously discussed, the ARIMA model does not do well in capturing the seasonality. As a result, we could say that within a given season (i.e Summer to Winter or Winter to Summer), the ARIMA models are optimal. But when we are looking to predict beyond a 6 month period, we should rely on sNaive or the SARIMA models."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA.html#california-energy-consumption",
    "href": "ARMA_ARIMA_SARIMA.html#california-energy-consumption",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "California Energy Consumption",
    "text": "California Energy Consumption\n\n\n\n\n\n\n\nDetermining ARIMA Parameters\n\n\n\n\n\nWe can tell right away that a sarima model is going to be necessary for this data as lags of 12, 24, 36, and 48 are all significant in the ACF plot, we will examine these models later. Given an Arima framework, since the data is already differenced and stationary d = 0. However, when we fit the model using the raw data we would set d = 1 to take the first difference of the data. For p, the candidates based on the PACF plot are 7-11 as these show significant autocorrelation (I will test 1-11). We stop at 11 because the period of the data is 12 so we should not use 12+ AR or MA terms. The candidates for q are truly 0 and would be really taken into account in the sarima model, but for now we will test 0-4.\n\nARIMA Model Selection\n\n\n\n\n \n  \n      \n    p \n    d \n    q \n    AIC \n    BIC \n    AICc \n  \n \n\n  \n    60 \n    11 \n    1 \n    4 \n    234.9708 \n    285.9689 \n    238.3288 \n  \n  \n    601 \n    11 \n    1 \n    4 \n    234.9708 \n    285.9689 \n    238.3288 \n  \n  \n    602 \n    11 \n    1 \n    4 \n    234.9708 \n    285.9689 \n    238.3288 \n  \n\n\n\n\n\nFrom our results, we can see that ARIMA(11,1,4) has the lowest values for all 3 metrics which makes our choice easy!\n\n\nARIMA Model Diagnostics\n\n\n\n\n\nLooking at the model diagnostics of ARIMA(11,1,4), we see that there is essentially no autocorrelation among the residuals according to the ACF. The Ljung-box statistic doesn’t quite confirms this, failing to reject the null at basically all lags, indicating some residual autocorrelation which is not ideal. Unfortunately, the Q-Q plot shows that the residuals are not entirely normally distributed - ideally they would be, which could indicate that there are better models for this data like sarima which takes into account seasonality.\n\n\nARIMA Model Equation\nGiven the model we can write our equation: \\[(1-B)y_t = c + (1 + \\phi_1 B + \\phi_2 B^2 + \\phi_3 B^3 + \\cdots + \\phi_{11} B^{11}) y_{t-1} + (1 + \\theta_1 B + \\theta_2 B^2 + \\theta_3 B^3 + \\theta_4 B^4)) \\varepsilon_t\\] where \\(B\\) is the lag operator, where \\(B^ky_t = y_{t-k}\\), \\(\\phi\\) represents the AR terms \\(\\theta\\) represents the noise (MA) terms, \\(\\varepsilon_t\\) represents the error, and \\(c\\) represents the intercept.\n\n\nARIMA Comparison to auto.arima()\n\n\nSeries: cons_orig_ts \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.6690  -0.9094\ns.e.  0.0695   0.0295\n\nsigma^2 = 0.4597:  log likelihood = -183.66\nAIC=373.33   AICc=373.46   BIC=382.89\n\n\nAuto.arima computed the optimal model as ARIMA(1,1,1) indicating that it would opt for a significantly simpler model then what we computed. Despite the jarring disagreement, this makes sense because of the results we get from our arima training. Remember that auto.arima moves in a stepwise fashion and will only continue if there is an improvement in the metrics. In our model training, the metrics actually got worse by adding AR terms before they got better. If we changed the argument stepwise to false, auto.arima will do an exhaustive search and return a different result.\n\n\nSeries: cons_orig_ts \nARIMA(11,1,3) \n\nCoefficients:\n          ar1      ar2      ar3      ar4      ar5      ar6      ar7      ar8\n      -0.6753  -0.7452  -0.8328  -0.7342  -0.7244  -0.7230  -0.7427  -0.7233\ns.e.   0.0871   0.0783   0.0644   0.0666   0.0672   0.0674   0.0656   0.0661\n          ar9     ar10     ar11     ma1     ma2     ma3\n      -0.7423  -0.7006  -0.6480  0.0718  0.3257  0.3495\ns.e.   0.0543   0.0593   0.0648  0.1164  0.0911  0.0982\n\nsigma^2 = 0.1976:  log likelihood = -106.39\nAIC=242.78   AICc=245.73   BIC=290.59\n\n\nOnce making this change auto.arima returned a ARIMA(11,1,3) model. Importantly, we also had to modify the max.order argument because auto.arima is anti complex models. It is pretty clear throughout this analysis that the model we are currently using the ARIMA(11,1,4) is really beyond the complexity bounds that we want - however, sarima models should take care of this easily when we get to them. auto.arima’s suggestion of ARIMA(11,1,3) is decently similar to ARIMA(11,1,4) as it justadds 1 MA term. However, given that ARIMA(11,1,4) has better metrics for all 3, we will continue with that model.\n\n\nMapping ARIMA to Actual Data\n\n\n\n\n\n\nHere we are plotting the predictions from the ARIMA model on the training data over the actual data. The model does a great job of staying close to the data which gives us confidence that this model can track solar energy consumption.\n\n\nARIMA Forecast\n\n\n\n\n\nPlotting the forecast we see a continued projection of the trend in the data as it had leveled out significantly since 2015. The forecast is for the next 48 months or 4 years and we see with the error bars how significantly this projection can vary. Given that we are looking at the log of the consumption data, a linear increase here is an exponential increase in real life. Unlike the GHI model, this one does show an ability to oscillate in the shorter term.\n\n\nCompare ARIMA to Benchmarks\n\n\n\n\n \n  \n    Benchmark \n    ME \n    RMSE \n    MAE \n    MPE \n    MAPE \n  \n \n\n  \n    Fitted ARIMA \n    0.05 \n    0.13 \n    0.1 \n    0.53 \n    1.19 \n  \n  \n    Mean \n    2.41 \n    2.43 \n    2.41 \n    27.49 \n    27.49 \n  \n  \n    Naive \n    -0.09 \n    0.34 \n    0.27 \n    -1.19 \n    3.18 \n  \n  \n    Drift \n    -0.44 \n    0.6 \n    0.48 \n    -5.18 \n    5.65 \n  \n  \n    sNaive \n    0.06 \n    0.15 \n    0.12 \n    0.73 \n    1.39 \n  \n\n\n\n\n\nThe train set is assigned as 2006-01-01 to 2019-04-01, with the test set being 2019-05-01 to 2020-12-01. Comparing the ARIMA model to benchmark methods over a 20 month prediction window, we see that the fitted ARIMA is significantly better than all other models except the sNaive which does a decent job.\n\n\n\n\n\n\nPlotting the predictions from the ARIMA model and the benchmark methods, we see the ARIMA model and sNaive are essentially in lockstep. They map so closely because the log of solar energy consumption has begun to level off and become more predictable. The slight increase in consumption over the test period is negligible in the log plot, but it is what gives the ARIMA the model the edge in terms of RMSE above. If we were to stop the analysis here and not consider a sarima model, it is conceivable that sNaive would be the better model to use simply due to the lack of computational complexity compared to the ARIMA model. The other benchmark methods are poorly suited to this data.\n\n\n\nSARIMA Model Parameters\n\n\n\n\n\nGiven our initial PACF and ACF plots of the differenced log consumption data, we see how many spikes there are at the seasonal lags of 12,24, 36, and 48. This indicates that we should attempt to do some seasonal differencing on the data and see what happens.\n\n\n\n\n\nAfter the seasonal differencing we see many of these spikes have disappeared and tightening up a lot of the autocorrelation of the lags which indicates this was a smart choice. Given this knowledge, we now have to make new parameters to search through for our SARIMA model. Here, our candidates for p are 1,2, the candidates for P are 0 (we will search 1 - 3). Our candidates for q based on the ACF are 1, and for Q we will examine 1. There is no harm in looking through these extra fits outside of computational power. Given that we performed both first and seasonal differencing d = 1, and D = 1.\n\nSARIMA Model Selection\n\n\n\n\n \n  \n      \n    p \n    d \n    q \n    P \n    D \n    Q \n    AIC \n    BIC \n    AICc \n  \n \n\n  \n    3 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    190.8402 \n    197.0762 \n    190.9134 \n  \n  \n    31 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    190.8402 \n    197.0762 \n    190.9134 \n  \n  \n    32 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    190.8402 \n    197.0762 \n    190.9134 \n  \n\n\n\n\n\nThe SARIMA(0,1,1)(0,1,0)[12] model returned the best value for all 3 metrics making the choice straightforward.\n\n\nSARIMA Model Diagnostics\n\n\n\n\n\nThese model diagnostics indicate precisely why this data was much better suited for a SARIMA model. Not only are the model evaluation metrics better but the residuals are much more normally distributed and exhibit less autocorrelation. At all lags we see a failure to reject the null given by the Ljung-Box test indicating that the residuals are not significantly autocorrelated which is exactly what we want to see. Also, it is important to note that this model has significantly fewer terms than the ARIMA(11,1,4) model which is also confirmation of the superior fit.\n\n\nSARIMA Model Equation\nThe SARIMA equation is \\[y_t = \\frac{\\theta_1 (1-B^{12})e_t}{(1-B^{12})(1-B)}\\] where \\(B\\) is the lag operator, where \\(B^ky_t = y_{t-k}\\), \\(\\theta\\) represents the noise (MA) terms, and \\(\\varepsilon_t\\) represents the error.\n\n\nSARIMA Comparison to auto.arima()\n\n\nSeries: cons_orig_ts \nARIMA(0,1,1)(0,1,0)[12] \n\nCoefficients:\n          ma1\n      -0.7811\ns.e.   0.0464\n\nsigma^2 = 0.1793:  log likelihood = -93.42\nAIC=190.84   AICc=190.91   BIC=197.08\n\n\nAuto.arima computed the optimal model as SARIMA(0,1,1)(0,1,0)[12] which is exactly what our model selection process returned indicating strong evidence that the SARIMA model is the optimal fit.\n\n\nMapping SARIMA to Actual Data\n\n\n\n\n\n\nHere we are plotting the predictions from the SARIMA model on the training data over the actual data. The model closely resembles the train fit of the ARIMA model and both do a great job of staying close to the data which gives us confidence that this model can track solar energy consumption.\n\n\nSARIMA Forecast\n\n\n\n\n\nThe SARIMA forecast is different then the ARIMA forecast in a few ways. First, the error bars over time become much larger which at first glance would indicate that the ARIMA forecast is a bit more certain of its projection then the SARIMA one. this is true, but the SARIMA forecast likely has a more accurate measure of uncertainty given that there is true seasonality in the data. We also see that the forecast projects sharper seasonal changes as opposed to the smoother peaks and troughs from ARIMA. This is because with the knowledge of seasonality, the SARIMA model expects the seasonal reversals to be more of a pivot rather than a gradual incline and decline, better resembling the past data.\n\n\nCompare SARIMA to Benchmarks\n\n\n\n\n \n  \n    Benchmark \n    ME \n    RMSE \n    MAE \n    MPE \n    MAPE \n  \n \n\n  \n    Fitted SARIMA \n    0.04 \n    0.14 \n    0.11 \n    0.48 \n    1.31 \n  \n  \n    Fitted ARIMA \n    0.05 \n    0.13 \n    0.1 \n    0.53 \n    1.19 \n  \n  \n    Mean \n    2.41 \n    2.43 \n    2.41 \n    27.49 \n    27.49 \n  \n  \n    Naive \n    -0.09 \n    0.34 \n    0.27 \n    -1.19 \n    3.18 \n  \n  \n    Drift \n    -0.44 \n    0.6 \n    0.48 \n    -5.18 \n    5.65 \n  \n  \n    sNaive \n    0.06 \n    0.15 \n    0.12 \n    0.73 \n    1.39 \n  \n\n\n\n\n\nUsing the same test and train set as before we see that the SARIMA model slightly underperforms the ARIMA model on the test set, but by a negligible amount. Given the fact that seasonality is truly present in the data from all of our analysis, the slight increase in inaccuracy is not enough to claim that the SARIMA is no longer valid.\n\n\n\n\n\n\nPlotting the predictions from the SARIMA model and the benchmark methods, we see the SARIMA, ARIMA, and sNaive models are essentially in lockstep. They map so closely because the log of solar energy consumption has begun to level off and become more predictable. The slight increase in consumption over the test period is negligible in the log plot, but it is what gives the ARIMA and SARIMA models the edge in terms of RMSE above. Given this information we can say that based on the slight increase in accuracy of the SARIMA model, our knowledge of seasonality in the data, and the vast decrease in computational complexity compared to ARIMA, that the SARIMA model is superior to the others. It is important to note that if the trends in solar energy consumption continue to remain this level, that the sNaive model will continue to perform extremely well. However, the SARIMA model will be more generalizable and effective at adjusting to changes, while maintaining low computational complexity."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA.html#solar-stocks---spwr",
    "href": "ARMA_ARIMA_SARIMA.html#solar-stocks---spwr",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Solar Stocks - SPWR",
    "text": "Solar Stocks - SPWR\nHere, we examine SPWR stock data which we have taken the log of and used first differencing to make stationary. Given that stock returns don’t show a seasonal pattern, we will be mainly focused on ARIMA models for this data and later we will look at PARCH and GARCH models.\n\n\n\n\n\n\n\nDetermining ARIMA Parameters\n\n\n\n\n\nGiven an Arima framework, since the data is already differenced and stationary d = 0. However, when we fit the model using the raw data we would set d = 1 to take the first difference of the data. For p, the candidates based on the PACF plot are 1,8 as these show significant autocorrelation (I will test 1-8). The candidates for q are also 1,8 so 1-8 will be tested.\n\n\nARIMA Model Selection\n\n\n\n\n \n  \n      \n    p \n    d \n    q \n    AIC \n    BIC \n    AICc \n  \n \n\n  \n    32 \n    3 \n    1 \n    4 \n    -12663.63 \n    -12613.74 \n    -12663.59 \n  \n  \n    1 \n    0 \n    1 \n    0 \n    -12662.08 \n    -12655.85 \n    -12662.08 \n  \n  \n    321 \n    3 \n    1 \n    4 \n    -12663.63 \n    -12613.74 \n    -12663.59 \n  \n\n\n\n\n\nThe relevant metrics suggest 2 models - ARIMA(3,1,4) and ARIMA(0,1,0). BIC suggested the simpler model as usual, but we will want to examine both. Before we proceed further it is worth addressing an ARIMA(0,1,0) model and what that indicates. This is a model with no AR terms, and no MA terms meaning that the next value in the series would be modeled as the previous value in the series plus random noise. Essentially, it means that there is no trend in the data and it is a random walk pattern.\n\n\nModel Diagnostics\n\n\n\n\n\n\n\n\nLooking at the model diagnostics of ARIMA(3,1,4), we see that there is essentially no autocorrelation among the residuals according to the ACF. The Ljung-box statistic largely confirms this, failing to reject the null at basically all lags, indicating a lack of residual autocorrelation which is good. Unfortunately, the Q-Q plot shows that the residuals are not entirely normally distributed. For ARIMA(0,1,0), there is no autocorrelation and the Ljung-box statistic fails to reject at every lag - otherwise the results are similar. Due to parsimony and the performance on the model diagnostics we will continue with ARIMA(0,1,0) the random walk model.\n\n\nModel Equation\nGiven the model we can write our equation: \\[(1-B)y_t=c+\\epsilon_t\\] where \\(B\\) is the lag operator, where \\(B^ky_t = y_{t-k}\\) and \\(c\\) represents the intercept. The intercept is expected to be 0 in a centered series (mean 0) like the one we are modeling.\n\n\nComparison to auto.arima()\n\n\nSeries: stock_orig_ts \nARIMA(0,1,0) \n\nsigma^2 = 0.002043:  log likelihood = 6332.04\nAIC=-12662.08   AICc=-12662.08   BIC=-12655.85\n\n\nAuto.arima computed the optimal model as ARIMA(0,1,0) which is in agreement with our own testing procedure. It is becoming apparent that a random walk model does well for predicting daily stock returns of SPWR. Usually this is a baseline model that can be improved upon.\n\n\nMapping to Actual Data\n\n\n\n\n\n\nHere we are plotting the predictions from the ARIMA model on the training data over the actual data. Because this model is so simplistic, the train fit values are simply the actual values but shifted back a period. If you zoom in on the graph you can see this for yourself.\n\n\nForecast\n\n\n\n\n\nThis is a forecast for the next 1000 trading days. We can see one of the many drawbacks of this random walk model clearly - that predictions are the past value plus a noise value with mean 0. The error bars we see in this case are the CI intervals as explained above but because of the form of this model, these also happen to represent a Gaussian distribution perfectly with mean zero and constant variance, as that is the only uncertainty parameter in the model.\n\n\nCompare to Benchmarks\n\n\n\n\n \n  \n    Benchmark \n    ME \n    RMSE \n    MAE \n    MPE \n    MAPE \n  \n \n\n  \n    Fitted ARIMA \n    0.67 \n    0.78 \n    0.68 \n    23.05 \n    23.24 \n  \n  \n    Mean \n    0.27 \n    0.47 \n    0.4 \n    8.05 \n    14.27 \n  \n  \n    Naive \n    0.67 \n    0.78 \n    0.68 \n    23.05 \n    23.24 \n  \n  \n    Drift \n    0.69 \n    0.79 \n    0.69 \n    23.55 \n    23.72 \n  \n  \n    sNaive \n    0.86 \n    1.05 \n    0.88 \n    28.68 \n    29.96 \n  \n\n\n\n\n\nThe train set is assigned as 2006-01-03 to 2020-08-08, with the test set being 2020-08-10 to 2020-12-30. Comparing the ARIMA model to benchmark methods over a 100 trading day prediction window, we see that the fitted ARIMA does exactly as well as the Naive model (because they are equivalent models, using the last entry as the predicted value), worse then the mean method (which will predict the mean stock price), and sightly better than the random walk with a drift term, and the sNaive model.\n\n\n\n\n\n\nPlotting the predictions from the ARIMA model and the benchmark methods, we see just how poorly the fit model does, and how unnatural it appears. The sNaive prediction looks natural and may have fit very well if the stock entered a downtrned just like it did at the end of 2019, but this would be no more than luck. For financial modeling, we need better tools, which we examine more in the Financial Time Series Models tab."
  },
  {
    "objectID": "ARMA_ARIMA_SARIMA.html#california-solar-energy-consumption",
    "href": "ARMA_ARIMA_SARIMA.html#california-solar-energy-consumption",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "California Solar Energy Consumption",
    "text": "California Solar Energy Consumption\nMoving on to California Solar Energy Consumption, we already have taken the log of the data and made it stationary, but can still see seasonality in the overall consumption data, indicating that we will explore both ARIMA and SARIMA models.\n\n\n\n\n\n\n\nDetermining ARIMA Parameters\n\n\n\n\n\nWe can tell right away that a sarima model is going to be necessary for this data as lags of 12, 24, 36, and 48 are all significant in the ACF plot, we will examine these models later. Given an Arima framework, since the data is already differenced and stationary d = 0. However, when we fit the model using the raw data we would set d = 1 to take the first difference of the data. For p, the candidates based on the PACF plot are 7-11 as these show significant autocorrelation (I will test 1-11). We stop at 11 because the period of the data is 12 so we should not use 12+ AR or MA terms. The candidates for q are truly 0 and would be really taken into account in the sarima model, but for now we will test 0-4.\n\nARIMA Model Selection\n\n\n\n\n \n  \n      \n    p \n    d \n    q \n    AIC \n    BIC \n    AICc \n  \n \n\n  \n    60 \n    11 \n    1 \n    4 \n    234.9708 \n    285.9689 \n    238.3288 \n  \n  \n    601 \n    11 \n    1 \n    4 \n    234.9708 \n    285.9689 \n    238.3288 \n  \n  \n    602 \n    11 \n    1 \n    4 \n    234.9708 \n    285.9689 \n    238.3288 \n  \n\n\n\n\n\nFrom our results, we can see that ARIMA(11,1,4) has the lowest values for all 3 metrics which makes our choice easy!\n\n\nARIMA Model Diagnostics\n\n\n\n\n\nLooking at the model diagnostics of ARIMA(11,1,4), we see that there is essentially no autocorrelation among the residuals according to the ACF. The Ljung-box statistic doesn’t quite confirms this, failing to reject the null at basically all lags, indicating some residual autocorrelation which is not ideal. Unfortunately, the Q-Q plot shows that the residuals are not entirely normally distributed - ideally they would be, which could indicate that there are better models for this data like sarima which takes into account seasonality.\n\n\nARIMA Model Equation\nGiven the model we can write our equation: \\[(1-B)y_t = c + (1 + \\phi_1 B + \\phi_2 B^2 + \\phi_3 B^3 + \\cdots + \\phi_{11} B^{11}) y_{t-1} + (1 + \\theta_1 B + \\theta_2 B^2 + \\theta_3 B^3 + \\theta_4 B^4)) \\varepsilon_t\\] where \\(B\\) is the lag operator, where \\(B^ky_t = y_{t-k}\\), \\(\\phi\\) represents the AR terms \\(\\theta\\) represents the noise (MA) terms, \\(\\varepsilon_t\\) represents the error, and \\(c\\) represents the intercept.\n\n\nARIMA Comparison to auto.arima()\n\n\nSeries: cons_orig_ts \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.6690  -0.9094\ns.e.  0.0695   0.0295\n\nsigma^2 = 0.4597:  log likelihood = -183.66\nAIC=373.33   AICc=373.46   BIC=382.89\n\n\nAuto.arima computed the optimal model as ARIMA(1,1,1) indicating that it would opt for a significantly simpler model then what we computed. Despite the jarring disagreement, this makes sense because of the results we get from our arima training. Remember that auto.arima moves in a stepwise fashion and will only continue if there is an improvement in the metrics. In our model training, the metrics actually got worse by adding AR terms before they got better. If we changed the argument stepwise to false, auto.arima will do an exhaustive search and return a different result.\n\n\nSeries: cons_orig_ts \nARIMA(11,1,3) \n\nCoefficients:\n          ar1      ar2      ar3      ar4      ar5      ar6      ar7      ar8\n      -0.6753  -0.7452  -0.8328  -0.7342  -0.7244  -0.7230  -0.7427  -0.7233\ns.e.   0.0871   0.0783   0.0644   0.0666   0.0672   0.0674   0.0656   0.0661\n          ar9     ar10     ar11     ma1     ma2     ma3\n      -0.7423  -0.7006  -0.6480  0.0718  0.3257  0.3495\ns.e.   0.0543   0.0593   0.0648  0.1164  0.0911  0.0982\n\nsigma^2 = 0.1976:  log likelihood = -106.39\nAIC=242.78   AICc=245.73   BIC=290.59\n\n\nOnce making this change auto.arima returned a ARIMA(11,1,3) model. Importantly, we also had to modify the max.order argument because auto.arima is anti complex models. It is pretty clear throughout this analysis that the model we are currently using the ARIMA(11,1,4) is really beyond the complexity bounds that we want - however, sarima models should take care of this easily when we get to them. auto.arima’s suggestion of ARIMA(11,1,3) is decently similar to ARIMA(11,1,4) as it justadds 1 MA term. However, given that ARIMA(11,1,4) has better metrics for all 3, we will continue with that model.\n\n\nMapping ARIMA to Actual Data\n\n\n\n\n\n\nHere we are plotting the predictions from the ARIMA model on the training data over the actual data. The model does a great job of staying close to the data which gives us confidence that this model can track solar energy consumption.\n\n\nARIMA Forecast\n\n\n\n\n\nPlotting the forecast we see a continued projection of the trend in the data as it had leveled out significantly since 2015. The forecast is for the next 48 months or 4 years and we see with the error bars how significantly this projection can vary. Given that we are looking at the log of the consumption data, a linear increase here is an exponential increase in real life. Unlike the GHI model, this one does show an ability to oscillate in the shorter term.\n\n\nCompare ARIMA to Benchmarks\n\n\n\n\n \n  \n    Benchmark \n    ME \n    RMSE \n    MAE \n    MPE \n    MAPE \n  \n \n\n  \n    Fitted ARIMA \n    0.05 \n    0.13 \n    0.1 \n    0.53 \n    1.19 \n  \n  \n    Mean \n    2.41 \n    2.43 \n    2.41 \n    27.49 \n    27.49 \n  \n  \n    Naive \n    -0.09 \n    0.34 \n    0.27 \n    -1.19 \n    3.18 \n  \n  \n    Drift \n    -0.44 \n    0.6 \n    0.48 \n    -5.18 \n    5.65 \n  \n  \n    sNaive \n    0.06 \n    0.15 \n    0.12 \n    0.73 \n    1.39 \n  \n\n\n\n\n\nThe train set is assigned as 2006-01-01 to 2019-04-01, with the test set being 2019-05-01 to 2020-12-01. Comparing the ARIMA model to benchmark methods over a 20 month prediction window, we see that the fitted ARIMA is significantly better than all other models except the sNaive which does a decent job.\n\n\n\n\n\n\nPlotting the predictions from the ARIMA model and the benchmark methods, we see the ARIMA model and sNaive are essentially in lockstep. They map so closely because the log of solar energy consumption has begun to level off and become more predictable. The slight increase in consumption over the test period is negligible in the log plot, but it is what gives the ARIMA the model the edge in terms of RMSE above. If we were to stop the analysis here and not consider a sarima model, it is conceivable that sNaive would be the better model to use simply due to the lack of computational complexity compared to the ARIMA model. The other benchmark methods are poorly suited to this data.\n\n\n\nSARIMA Model Parameters\n\n\n\n\n\nGiven our initial PACF and ACF plots of the differenced log consumption data, we see how many spikes there are at the seasonal lags of 12,24, 36, and 48. This indicates that we should attempt to do some seasonal differencing on the data and see what happens.\n\n\n\n\n\nAfter the seasonal differencing we see many of these spikes have disappeared and tightening up a lot of the autocorrelation of the lags which indicates this was a smart choice. Given this knowledge, we now have to make new parameters to search through for our SARIMA model. Here, our candidates for p are 1,2, the candidates for P are 0 (we will search 1 - 3). Our candidates for q based on the ACF are 1, and for Q we will examine 1. There is no harm in looking through these extra fits outside of computational power. Given that we performed both first and seasonal differencing d = 1, and D = 1.\n\nSARIMA Model Selection\n\n\n\n\n \n  \n      \n    p \n    d \n    q \n    P \n    D \n    Q \n    AIC \n    BIC \n    AICc \n  \n \n\n  \n    3 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    190.8402 \n    197.0762 \n    190.9134 \n  \n  \n    31 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    190.8402 \n    197.0762 \n    190.9134 \n  \n  \n    32 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    190.8402 \n    197.0762 \n    190.9134 \n  \n\n\n\n\n\nThe SARIMA(0,1,1)(0,1,0)[12] model returned the best value for all 3 metrics making the choice straightforward.\n\n\nSARIMA Model Diagnostics\n\n\n\n\n\nThese model diagnostics indicate precisely why this data was much better suited for a SARIMA model. Not only are the model evaluation metrics better but the residuals are much more normally distributed and exhibit less autocorrelation. At all lags we see a failure to reject the null given by the Ljung-Box test indicating that the residuals are not significantly autocorrelated which is exactly what we want to see. Also, it is important to note that this model has significantly fewer terms than the ARIMA(11,1,4) model which is also confirmation of the superior fit.\n\n\nSARIMA Model Equation\nThe SARIMA equation is \\[y_t = \\frac{\\theta_1 (1-B^{12})e_t}{(1-B^{12})(1-B)}\\] where \\(B\\) is the lag operator, where \\(B^ky_t = y_{t-k}\\), \\(\\theta\\) represents the noise (MA) terms, and \\(\\varepsilon_t\\) represents the error.\n\n\nSARIMA Comparison to auto.arima()\n\n\nSeries: cons_orig_ts \nARIMA(0,1,1)(0,1,0)[12] \n\nCoefficients:\n          ma1\n      -0.7811\ns.e.   0.0464\n\nsigma^2 = 0.1793:  log likelihood = -93.42\nAIC=190.84   AICc=190.91   BIC=197.08\n\n\nAuto.arima computed the optimal model as SARIMA(0,1,1)(0,1,0)[12] which is exactly what our model selection process returned indicating strong evidence that the SARIMA model is the optimal fit.\n\n\nMapping SARIMA to Actual Data\n\n\n\n\n\n\nHere we are plotting the predictions from the SARIMA model on the training data over the actual data. The model closely resembles the train fit of the ARIMA model and both do a great job of staying close to the data which gives us confidence that this model can track solar energy consumption.\n\n\nSARIMA Forecast\n\n\n\n\n\nThe SARIMA forecast is different then the ARIMA forecast in a few ways. First, the error bars over time become much larger which at first glance would indicate that the ARIMA forecast is a bit more certain of its projection then the SARIMA one. this is true, but the SARIMA forecast likely has a more accurate measure of uncertainty given that there is true seasonality in the data. We also see that the forecast projects sharper seasonal changes as opposed to the smoother peaks and troughs from ARIMA. This is because with the knowledge of seasonality, the SARIMA model expects the seasonal reversals to be more of a pivot rather than a gradual incline and decline, better resembling the past data.\n\n\nOne step ahead and 12 step ahead forecasting\n\n\n\n\n \n  \n    Forecast \n    RMSE \n  \n \n\n  \n    1 Step Ahead Forecast \n    0.44 \n  \n  \n    365 Step Ahead Forecast \n    0.49 \n  \n\n\n\n\n\nUsing Cross Validation we can get a better look at the performance of this model with differing amounts of data. A time series cross validation works by doing h-step ahead forecasting starting with one data point and going until you are fitting the model with n-h data points and predicting h steps ahead. We see here that the model does better with 1 step ahead forecasting then with 12 step ahead forecasting, which should always be the case. The fact that they performed reasonably similarly indicates that this model retains similar utility over larger prediction windows which is a big plus.\n\n\n\n\n\n\nThis plot elucidates the point that SARIMA models do worse over larger prediction windows - this makes sense as we know the most relevant data points to a prediction are the most recent ones. The farther away the train data gets from the prediction, the less accurate it is likely to be.\n\n\nCompare SARIMA to Benchmarks\n\n\n\n\n \n  \n    Benchmark \n    ME \n    RMSE \n    MAE \n    MPE \n    MAPE \n  \n \n\n  \n    Fitted SARIMA \n    0.04 \n    0.14 \n    0.11 \n    0.48 \n    1.31 \n  \n  \n    Fitted ARIMA \n    0.05 \n    0.13 \n    0.1 \n    0.53 \n    1.19 \n  \n  \n    Mean \n    2.41 \n    2.43 \n    2.41 \n    27.49 \n    27.49 \n  \n  \n    Naive \n    -0.09 \n    0.34 \n    0.27 \n    -1.19 \n    3.18 \n  \n  \n    Drift \n    -0.44 \n    0.6 \n    0.48 \n    -5.18 \n    5.65 \n  \n  \n    sNaive \n    0.06 \n    0.15 \n    0.12 \n    0.73 \n    1.39 \n  \n\n\n\n\n\nUsing the same test and train set as before we see that the SARIMA model slightly underperforms the ARIMA model on the test set, but by a negligible amount. Given the fact that seasonality is truly present in the data from all of our analysis, the slight increase in inaccuracy is not enough to claim that the SARIMA is no longer valid.\n\n\n\n\n\n\nPlotting the predictions from the SARIMA model and the benchmark methods, we see the SARIMA, ARIMA, and sNaive models are essentially in lockstep. They map so closely because the log of solar energy consumption has begun to level off and become more predictable. The slight increase in consumption over the test period is negligible in the log plot, but it is what gives the ARIMA and SARIMA models the edge in terms of RMSE above. Given this information we can say that based on the slight increase in accuracy of the SARIMA model, our knowledge of seasonality in the data, and the vast decrease in computational complexity compared to ARIMA, that the SARIMA model is superior to the others. It is important to note that if the trends in solar energy consumption continue to remain this level, that the sNaive model will continue to perform extremely well. However, the SARIMA model will be more generalizable and effective at adjusting to changes, while maintaining low computational complexity."
  },
  {
    "objectID": "SARIMAX.html",
    "href": "SARIMAX.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "In this section our focus is on fitting SARIMAX models to our data. These models are used to identify relationships between the time series and other variables by adding additional regressors to the SARIMA equation. One of our main questions of interest was how does weather (e.g., temperature, precipitation, cloud cover) in California affect solar power generation over time? We can use SARIMAX models to examine this relationship."
  },
  {
    "objectID": "SARIMAX.html#fitting-the-model-manually",
    "href": "SARIMAX.html#fitting-the-model-manually",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Fitting the Model Manually",
    "text": "Fitting the Model Manually\n\n\n\nCall:\nlm(formula = consumption ~ GHI + Dew.Point + Wind.Speed + Relative.Humidity + \n    Temperature + Precipitable.Water, data = df)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-4.439 -1.208  0.073  1.323  3.791 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         2.223e+01  3.693e+00   6.019 1.02e-08 ***\nGHI                 3.258e-05  5.300e-03   0.006  0.99510    \nDew.Point           7.924e-01  1.609e-01   4.924 1.97e-06 ***\nWind.Speed         -4.673e-01  6.056e-01  -0.772  0.44132    \nRelative.Humidity  -1.875e-01  4.244e-02  -4.419 1.75e-05 ***\nTemperature        -3.820e-01  1.304e-01  -2.930  0.00385 ** \nPrecipitable.Water -8.302e-01  8.934e-01  -0.929  0.35408    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.672 on 173 degrees of freedom\nMultiple R-squared:  0.2742,    Adjusted R-squared:  0.249 \nF-statistic: 10.89 on 6 and 173 DF,  p-value: 2.832e-10\n\n\nWe see that the variables GHI, Wind Speed, and Precipitable Water are not significant indicating that solar radiation, wind, and rain are not important predictors of solar energy consumption while Dew Point, Humidity, and Temperature are. We will now fit the model again using these variables.\n\n\n\nCall:\nlm(formula = consumption ~ Dew.Point + Relative.Humidity + Temperature, \n    data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.2710 -1.2788  0.0242  1.3516  3.6591 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        22.9574     3.6024   6.373 1.57e-09 ***\nDew.Point           0.7668     0.1382   5.549 1.04e-07 ***\nRelative.Humidity  -0.2068     0.0400  -5.170 6.30e-07 ***\nTemperature        -0.4891     0.1127  -4.341 2.39e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.672 on 176 degrees of freedom\nMultiple R-squared:  0.2624,    Adjusted R-squared:  0.2498 \nF-statistic: 20.87 on 3 and 176 DF,  p-value: 1.291e-11\n\n\n\n\n\n\n\nWe see extreme autocorrelation here amongst the residuals, so now we will difference them.\n\n\n\n\n\nThere is still autocorrelation so now we will apply seasonal differencing.\n\n\n\n\n\nThis looks better and is now ready for testing various SARIMA models. Based on the plots we should try p of 1-3, P of 1, q of 1-4, and Q of 1.\n\n\n\n\n \n  \n      \n    p \n    d \n    q \n    P \n    D \n    Q \n    AIC \n    BIC \n    AICc \n  \n \n\n  \n    60 \n    2 \n    1 \n    4 \n    1 \n    1 \n    1 \n    393.9246 \n    421.9865 \n    395.0711 \n  \n  \n    6 \n    0 \n    1 \n    1 \n    0 \n    1 \n    1 \n    396.1907 \n    405.5446 \n    396.3379 \n  \n  \n    601 \n    2 \n    1 \n    4 \n    1 \n    1 \n    1 \n    393.9246 \n    421.9865 \n    395.0711 \n  \n\n\n\n\n\n\nLooking at Model Diagnostics\n\n\n\n\n\n\n\n\nBoth model diagnostic plots look great so we should continue with SARIMA(0,1,1)(0,1,1)[12] due to the principle of parsimony as it is a much simpler model."
  },
  {
    "objectID": "SARIMAX.html#fitting-the-model-using-auto.arima",
    "href": "SARIMAX.html#fitting-the-model-using-auto.arima",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Fitting the Model Using auto.arima()",
    "text": "Fitting the Model Using auto.arima()\n\n\nSeries: df.ts[, \"consumption\"] \nRegression with ARIMA(0,1,1)(2,0,2)[12] errors \n\nCoefficients:\n          ma1    sar1    sar2    sma1    sma2  Dew.Point  Relative.Humidity\n      -0.7572  0.7015  0.0405  0.1520  0.2887     0.0257            -0.0147\ns.e.   0.0626  0.2189  0.2000  0.2129  0.1147     0.0356             0.0106\n      Temperature\n          -0.0197\ns.e.       0.0365\n\nsigma^2 = 0.1617:  log likelihood = -95.85\nAIC=209.7   AICc=210.76   BIC=238.38\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.02252101 0.3919819 0.2288218 -2.262883 6.992691 0.5976852\n                  ACF1\nTraining set 0.0456568\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,1)(2,0,2)[12] errors\nQ* = 27.498, df = 19, p-value = 0.09357\n\nModel df: 5.   Total lags used: 24\n\n\nauto.arima() fit a SARIMA(0,1,1)(2,0,2)[12] model. The residuals plot indicates that there is autocorrelation amongst the lags as the Ljung-Box test returned a p-value of .094. However, this number is close to the rejection criteria and the residuals plots do look solid."
  },
  {
    "objectID": "SARIMAX.html#choosing-the-best-model-with-cv",
    "href": "SARIMAX.html#choosing-the-best-model-with-cv",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Choosing the Best model with CV",
    "text": "Choosing the Best model with CV\n\n\n\n\n\n\nFit1 was the simple model chosen from manual selection and fit3 was the choice from auto.arima(). I also checked the other model from manual selection as fit2 to be thorough. Based on the results, we see that fit2 suffers from high variance and high bias, further indicating that this is not the best model. We see that fit3 is the best.\n\n\nSeries: df.ts[, \"consumption\"] \nRegression with ARIMA(0,1,1)(2,0,2)[12] errors \n\nCoefficients:\n          ma1    sar1    sar2    sma1    sma2  Dew.Point  Relative.Humidity\n      -0.7572  0.7015  0.0405  0.1520  0.2887     0.0257            -0.0147\ns.e.   0.0626  0.2189  0.2000  0.2129  0.1147     0.0356             0.0106\n      Temperature\n          -0.0197\ns.e.       0.0365\n\nsigma^2 = 0.1617:  log likelihood = -95.85\nAIC=209.7   AICc=210.76   BIC=238.38\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.02252101 0.3919819 0.2288218 -2.262883 6.992691 0.5976852\n                  ACF1\nTraining set 0.0456568"
  },
  {
    "objectID": "SARIMAX.html#forecasting",
    "href": "SARIMAX.html#forecasting",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Forecasting",
    "text": "Forecasting\n\n\nSeries: df$Dew.Point \nARIMA(1,0,0)(1,0,0)[12] with non-zero mean \n\nCoefficients:\n         ar1    sar1    mean\n      0.3275  0.1966  5.3102\ns.e.  0.0743  0.0781  0.2870\n\nsigma^2 = 4.568:  log likelihood = -390.91\nAIC=789.82   AICc=790.05   BIC=802.59\n\nTraining set error measures:\n                       ME     RMSE      MAE      MPE     MAPE      MASE\nTraining set -0.004585208 2.119434 1.673436 7.775676 62.35839 0.7885715\n                     ACF1\nTraining set -0.007595889\n\n\nSeries: df$Relative.Humidity \nARIMA(1,0,1)(1,1,1)[12] \n\nCoefficients:\n         ar1      ma1     sar1     sma1\n      0.7880  -0.4072  -0.0523  -0.8694\ns.e.  0.0958   0.1397   0.0982   0.0992\n\nsigma^2 = 57.18:  log likelihood = -585.29\nAIC=1180.57   AICc=1180.94   BIC=1196.19\n\nTraining set error measures:\n                     ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -0.7549294 7.217696 5.529367 -3.141838 10.76834 0.6587356\n                    ACF1\nTraining set -0.01212801\n\n\nSeries: df$Temperature \nARIMA(1,0,0)(2,1,1)[12] \n\nCoefficients:\n         ar1    sar1     sar2     sma1\n      0.3755  0.0884  -0.1265  -0.7675\ns.e.  0.0786  0.1170   0.1033   0.1014\n\nsigma^2 = 2.101:  log likelihood = -304.44\nAIC=618.89   AICc=619.26   BIC=634.51\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.2453951 1.383605 1.030662 0.9026313 6.698278 0.7491152\n                    ACF1\nTraining set -0.06335115\n\n\n\n\nWarning in forecast.forecast_ARIMA(fit, xreg = fxreg): xreg contains different\ncolumn names from the xreg used in training. Please check that the regressors\nare in the same order."
  },
  {
    "objectID": "SARIMAX.html#comparing-to-benchmarks",
    "href": "SARIMAX.html#comparing-to-benchmarks",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Comparing to benchmarks",
    "text": "Comparing to benchmarks\n\n\n\n\n \n  \n    Benchmark \n    ME \n    RMSE \n    MAE \n    MPE \n    MAPE \n  \n \n\n  \n    Fitted SARIMAX \n    0.16 \n    0.25 \n    0.2 \n    1.73 \n    2.26 \n  \n  \n    Fitted SARIMA \n    0.04 \n    0.14 \n    0.11 \n    0.48 \n    1.31 \n  \n  \n    Fitted ARIMA \n    0.05 \n    0.13 \n    0.1 \n    0.53 \n    1.19 \n  \n  \n    Mean \n    2.41 \n    2.43 \n    2.41 \n    27.49 \n    27.49 \n  \n  \n    Naive \n    -0.09 \n    0.34 \n    0.27 \n    -1.19 \n    3.18 \n  \n  \n    Drift \n    -0.44 \n    0.6 \n    0.48 \n    -5.18 \n    5.65 \n  \n  \n    sNaive \n    0.06 \n    0.15 \n    0.12 \n    0.73 \n    1.39 \n  \n\n\n\n\n\nWhat we see from these results is that the SARIMAX model is not the best for predicting solar energy consumption. Using the variables of dew point, temperature and humidity did not add to the predictability of consumption indicating that they are not useful in addition to the time series information itself, rather they add noise that reduces accuracy. This would indicate that these variables are not what causes changes in solar energy consumption which is arguably a positive sign. Solar energy consumption is directly tied to generation, so the fact that these weather related variables do not cause a significant impact means that weather is not a major factor - which is great if you want a consistent supply of energy to consume. Rather it appears that things like technological improvement, macroeconomic factors, and government policies are the driving force behind solar energy consumption changes"
  },
  {
    "objectID": "Financial.html",
    "href": "Financial.html",
    "title": "Financial Time Series Models",
    "section": "",
    "text": "As we saw in the ARIMA/SARIMA section, fitting time series models to stock data can be very difficult because daily returns tend to take on a random walk pattern. This is exactly what happened when we fit models manually and using auto.arima() - we used an ARIMA(0,1,0) model. This is not helpful in terms of predicting the next move as it provides no information.\nTo handle stock returns data more effectively, we have to look at ARCH and GARCH models which look at the volatility of returns, modeling the conditional variance."
  },
  {
    "objectID": "Financial.html#spwr-stock",
    "href": "Financial.html#spwr-stock",
    "title": "Financial Time Series Models",
    "section": "SPWR Stock",
    "text": "SPWR Stock\n\n\n\n\n\n\nTaking a look at the candlestick plot of SPWR, we see that there are periods of increased and decreased momentum, as well some flat and stable periods. When modeling the ARIMA model, we looked at the log returns.\n\n\n\n\n\nHere, we can see that the series is stationary, but we also see some outsized returns (both positve and negative) that seem to cluster around eachother, this phenomenon is known as volatility clustering.\n\nModeling the Stock Returns\n\n\n\n\n\nLooking at the ACF and PACF as before, we see no significant autocorrelation - a major part of the reason why we couldn’t model the future with anything more than a random walk.\n\n\nLooking at Squared Returns\nHowever, when we look at the squared returns, we see a different picture.\n\n\n\n\n\n\n\n\n\n\n\nThe squared returns do have significant autocorrelation which means that this volatility clustering we saw above is real in the stock. Based on both the ACF and PACF, it seems that we need to model this with a GARCH model, specifiying both p and q for the returns. This is because we see persistence in the autocorrelation. It is important to note that normally you would fit an ARIMA model on the returns and then a GARCH model on the residuals, but in the case of the returns being modeled by ARIMA (0,1,0), we can just difference the data and fit the GARCH model.\nBased on the plots, we will test p’s of 1-8, and q’s of 1-7.\n\n\nTesting GARCH Models\n\n\n\n\n \n  \n      \n    p \n    q \n    AIC \n    BIC \n  \n \n\n  \n    7 \n    1 \n    6 \n    -3.495232 \n    -3.480361 \n  \n  \n    3 \n    1 \n    2 \n    -3.492599 \n    -3.484338 \n  \n\n\n\n\n\nThe best model according to AIC is garch(1,6) and BIC chooses garch(1,2). Given the close performance between the two and the simplicity of garch(1,2), that is the model we will continue with.\n\n\nExamining GARCH(1,2)\n\n\n\n\n\n\n    Box-Pierce test\n\ndata:  residuals(arch12)\nX-squared = 0.0028994, df = 1, p-value = 0.9571\n\n\nWe see two things from the information above. First, the residuals look great - no autocorrelation and normally distributed. We also ran the Ljung-Box test on the residuals which we have been using as a significance test to accept or reject the null that the residuals are not autocorrelated and we get a p-value of 0.96, indicating we can accept with confidence.\n\n\nWriting the Model Equation\nGiven the GARCH(1,2) fit we can write our equation as: \\[\ny_t &= \\mu_t + \\epsilon_t &\\\n\\epsilon_t &= \\sigma_t z_t &\\\n\\sigma_t^2 &= \\omega + \\alpha_1 \\epsilon_{t-1}^2 + \\alpha_2 \\epsilon_{t-2}^2 + \\beta_1 \\sigma_{t-1}^2 + \\beta_2 \\sigma_{t-2}^2\n\\end{align*} \\]\nwhere:\n\\(y_t\\) is the observed time series at time \\(t\\), \\(\\mu_t\\) is the conditional mean of \\(y_t\\) at time \\(t\\),\\(\\epsilon_t\\) is the standardized residual of \\(y_t\\) at time \\(t\\) \\(\\sigma_t^2\\) is the conditional variance of \\(\\epsilon_t\\) at time \\(t\\) \\(z_t\\) is a standard normal random variable \\(\\omega\\) is the constant term in the GARCH model \\(\\alpha_1\\), \\(\\alpha_2\\), \\(\\beta_1\\), and \\(\\beta_2\\) are the autoregressive parameters of the GARCH model\n\n\nPlotting Variance\n\n\n\n\n\n\nLooking at the volatility graph from the fitted model, it correlates quite well to the periods of sharp increase and decrease we see in the stock overall, namely around 2008 (the GFC), 2013 (the year leading up to when the California Solar Initiative took effect), 2016, a period of sharp decline for the stock, and the last spike around COVID.\n\n\nForecast\n\n\n\n\n\n\nSo with this plot what we get as improvement over what the ARIMA model gave us is more confidence around what the returns are going to be, especially in the short term. With this knowledge of variance, we can compare it to other stocks to make investment decisions, manage risks, etc. It is important to note that these models need constant updating, and are really only reliable tools for a very short future horizon. As a result, these models are very useful for short term trading and options trading."
  }
]